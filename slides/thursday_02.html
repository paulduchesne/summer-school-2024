<!DOCTYPE html>
<html>
  <head>
    <title>Digital Summer School 2024: THU02</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="../style.css">  </head>
  <body>
    <textarea id="source">




class: center, middle, titlepage
### THU02: *FFmpeg automation and optimisation*
---
class: contentpage
### **Agenda**

1. Building FFmpeg workflows in Python  
  1.1 PyPi packages: ffmpeg-python  
  1.2 Subprocess module  
  1.3 Metadata triggers for FFmpeg commands  
  1.4 Validating your files  
  1.5 Capturing and using filter data  
  
2. FFmpeg encoding optimisation   
  2.1 Hardware configurations    
  2.2 Processing  
  2.3 GNU Parallel  
  2.4 Python parallelisation and multiprocessing  
  2.5 GPU acceleration  
 
---
class: contentpage
### **1. Building FFmpeg workflows in Python**

Combining Python and FFmpeg is merging two of the most powerful tool available to AV digital preservation  

.centre[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/logos_combined.png" width="1000">]

With great power come great responsibility!

---
class: contentpage
### **1.1 PyPi packages: ffmpeg-python**

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/pypi_ffmpeg.png" width="840">]

---
class: contentpage
### **1.1 PyPi packages: ffmpeg-python**

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-20 at 17.54.56.png" width="700">]

It's worth checking how active a project is by viewing source code. The [ffmpeg-python GitHub](https://github.com/kkroening/ffmpeg-python/) is very active with recent Issues raised, many asking for help converting FFmpeg commands into python.

---
class: tangentpage
### **Tangent: Python Virtualenv**

For these FFmpeg and Python code tests it might be best to set up a separate Python virtual environment. If you haven't already then install virtualenv:
```sh
pip install virtualenv
```
Create a folder somewhere easy to locate named in a way that's easy to remember what the environment project represents:
```sh
mkdir ENV
python -m venv ENV/
```
Once your environment is live your command prompt will be prefixed with the Virtualenv folder name, eg (ENV). Now you can make your Python PyPi installations and run those installations in the REPL. Openn it with `python3` and type `quit()` to exit:
```sh
pip install ffmpeg-python
python3
quit()
```
REPL stands for Read Evaluate Print Loop, and it is useful for testing short pieces of code or package installations

---
class: contentpage
### **1.1 PyPi packages: ffmpeg-python**

Configure your Python environment for safe pip installation, and load up a Python REPL  
```sh
pip install ffmpeg-python
python3
```
<font color="orange">Practise:</font> Use the software to run a horizontal flip of a File
```python3
import ffmpeg
stream = ffmpeg.input('input.mp4')
stream = ffmpeg.hflip(stream)
stream = ffmpeg.output(stream, 'output.mp4')
ffmpeg.run(stream)
```

Or put together the command:
```python3
(
    ffmpeg
    .input('input.mp4')
    .hflip()
    .output('output.mp4')
    .run()
)
```
---
class: contentpage
### **1.1 PyPi packages: ffmpeg-python**

Using the example from the official [FFmpeg Python Github](https://github.com/kkroening/ffmpeg-python) for this package you can see it's possible to build some sophisticated commands:

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-17 at 22.18.12.png" width="800">]

The equivalent FFmpeg CLI command for this:

```sh
ffmpeg -i input.mp4 -i overlay.png -filter_complex "[0]trim=start_frame=10:end_frame=20[v0];\
    [0]trim=start_frame=30:end_frame=40[v1];[v0][v1]concat=n=2[v2];[1]hflip[v3];\
    [v2][v3]overlay=eof_action=repeat[v4];[v4]drawbox=50:50:120:120:red:t=5[v5]"\
    -map [v5] output.mp4
```
---
class: contentpage
### **1.1 PyPi packages: ffmpeg-python**

Complex FFmpeg layout becomes easier to read in a Pythonic way:

```python3
import ffmpeg

in_file = ffmpeg.input('input.mp4')
overlay_file = ffmpeg.input('overlay.png')
(
    ffmpeg
    .concat(
        in_file.trim(start_frame=10, end_frame=20),
        in_file.trim(start_frame=30, end_frame=40),
    )
    .overlay(overlay_file.hflip())
    .drawbox(50, 50, 120, 120, color='red', thickness=5)
    .output('out.mp4')
    .run()
)
```
```sh
ffmpeg -i input.mp4 -i overlay.png -filter_complex "[0]trim=start_frame=10:end_frame=20[v0];\
    [0]trim=start_frame=30:end_frame=40[v1];[v0][v1]concat=n=2[v2];[1]hflip[v3];\
    [v2][v3]overlay=eof_action=repeat[v4];[v4]drawbox=50:50:120:120:red:t=5[v5]"\
    -map [v5] output.mp4
```
---
class: contentpage
### **1.1 PyPi packages: ffmpeg-python**

There's also great reporting using this package to retrieve file metadata:
```python3
result = ffmpeg.probe('input.mov')
print(result)
```
You are returned with a formatted Python dictionary containing all your metadata. To get this data for, say the duration of the file:

```python3
duration = result.get('format', {}).get('duration', None)
print(duration)
```

<font color="orange">Practise:</font> Use the examples commmands above to query the 'streams' and 'format' dictionary keys, containing stream metadata for the video and audio:
- filename
- format_name
- start_time
- size
- bit_rate
- probe_score
- tags

---
class: contentpage
### **1.2 Subprocess module**

Python's subprocess module allows you to spawn new processes and connect with the stdin, stdout and strerr pipes. Where possible it's suggested you shuold use the `run()` function. The accepted arguments include:    

```python3
subprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None,\
               capture_output=False, shell=False, cwd=None, timeout=None, check=False,\
               encoding=None, errors=None, text=None, env=None, universal_newlines=None,\
               **other_popen_kwargs)
```

When you run this code `subprocess` will carry out the `args` as your command. If this is a list, you will need to include `shell=False`. But if you supply a string of your ffmpeg command you can set `shell=True`

```python3
result = subprocess.run('ffmpeg -i input.mov output.mp4', shell=True)
result = subprocess.run(['ffmpeg', '-i', 'input.mov', 'output.mp4'], shell=False)
```

<font color="red">Warning!</font> It is not recommended to use `shell=True`, as code can be vulnerable to [shell injection threats](https://docs.python.org/3/library/subprocess.html#security-considerations). If the command includes user inputs then users may inject commands to run that could compromise a system. 

---
class: contentpage
### **1.2 Subprocess module**

`subprocess.run()` can also be used to capture results of FFprobe metadata queries
```python3
response = subprocess.run(
  [
    'ffprobe', '-v', 'error', '-select_streams', 'v:0', '-show_entries',
    'stream=bit_rate', '-of', 'default=noprint_wrappers=1', 'input.mov'
  ],
  shell=False,
  capture_output=True,
  text=True
)
```
The FFrobe command is supplied as a list of strings, follow by subprocess argument `shell=False`, `capture_output=True` to ensure the response is captured and `text=True` ensures it's formatted as text and not bytes. It returns a 'CompletedProcess' class that allows the data to be viewed with these commands:

<font color="orange">Practise:</font> In the REPL `import subprocess` then try an FFprobe command and expore the responses
```python3
response.args
response.stdout
response.stderr
response.returncode
```
---
class: contentpage
### **1.2 Subprocess module**

Subprocess works for any open-source tool you call from the command line, like MediaInfo:

```python
duration = subprocess.run(
  [
    'mediainfo', '-i', '--Output=Video;%Format%', 'input.mkv'
  ],
  shell=False,
  capture_output=True,
  text=True
)
```
When you isolate specific metadata like width and height, you can start using it in the code to calculate display aspect ratios:

```python3
width = subprocess.run(['mediainfo', '-i', '--Output=Video;%Width%', 'input.mkv'], shell=False, capture_output=True, text=True)
height = subprocess.run(['mediainfo', '-i', '--Output=Video;%Height%', 'input.mkv'], shell=False, capture_output=True, text=True)

if "1.777" in str(int(width.stdout) / int(height.stdout)):
    aspect = '16x9'
```

---
class: contentpage
### **1.3 Metadata triggers for FFmpeg commands**

Using `ffmpeg-python` package we can retrieve colour information for a lossless video transcode and map into the FFmpeg command:

```python3
import ffmpeg

metadata = ffmpeg.probe('input.mkv')
cspace = metadata['streams'][0].get('color_space', '')
ctransfer = metadata['streams'][0].get('color_transfer', '')
cprimaries = metadata['streams'][0].get('color_primaries', '')
pformat = metadata['streams'][0].get('pix_fmt', '')

command = [
  'ffmpeg', '-i', 'input.mkv', '-c:v', 'v210', '-pix_fmt', pformat,
  '-color_primaries', cprimaries, '-color_trc', ctransfer,
  '-colorspace', cspace, '-c:a', 'copy', '-map', '0', 'output.mov'
]

encode = subprocess.run(command, shell=False, capture_output=True, text=True)
if encode.returncode != 0:
    # Problem encoding file
    print(encode.stderr)
```
---
class: contentpage
### **1.3 Metadata triggers for FFmpeg commands**

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-18 at 20.18.58.png" width="710">]

---
class: contentpage
### **1.3 Metadata triggers for FFmpeg commands**

<font color="orange">Practise:</font> Open the Python REPL in your repository folder `thursday_scripts`. Import command_builder.py and run functions below. You will need a input file path and output folder or file path to test the encoding command.
```python3
python3
import command_builder as cb

# Experiment with width, height, DAR, PAR possibilities to see the commands that are returned
command = cb.create('input.mov', 'output.mov', '1920', '1080', '16x9', '16x9')
print(command)
```
Use the `command` variable and paste into the next function:
```python3
success = cb.run(command)
print(success)
```

Hang on to this `success` variable for our next practise
---
class: contentpage
### **1.3 Metadata triggers for FFmpeg commands**

MediaInfo 0.0.9 returns a limited dictionary of metadata from this Mediainfo wrapper:
```python3
pip install MediaInfo

from MediaInfo install MediaInfo

info = MediaInfo(filename = 'input.mov')
info = MediaInfo(filename = 'input.mov', cmd = '/opt/homebrew/bin/ffprobe')
print(info.getInfo())
```

Example output for an MP4 file:
```python3
{'container': 'MPEG-4', 'fileSize': '57831125', 'duration': '628.139', 'bitrate': '736539',
'haveVideo': 1, 'haveAudio': 1, 'videoCodec': 'AVC', 'videoCodecProfile': 'High',
'videoDuration': '628.120', 'videoBitrate': '604037', 'videoWidth': 1920, 'videoHeight': 1080,
'videoAspectRatio': '1.778', 'videoFrameRate': '25.000', 'videoFrameCount': '15703',
'audioCodec': 'AAC', 'audioCodecProfile': 'LC', 'audioDuration': '628.139',
'audioBitrate': '126420', 'audioChannel': '2', 'audioSamplingRate': '48000'}
```

---
class: contentpage
### **1.4 Validating your files**

You can call MediaConch to validate a transcode file straight after encoding to check the file is healthy:
```python3
cmd = ['mediaconch', '-p', 'ffv1_policy.xml', 'input.mkv']
validated = subprocess.run(cmd, shell=False, capture_output=True, text=True)
if validated.stdout.startswith('pass!'):
    # passed!
```

FFrobe will give a very basic true/false response for an AV file's health:
```python3
cmd = ['ffprobe', '-i', fpath, '-loglevel', '-8']
check = subprocess.run(cmd, shell=False)
if check.returncode != 0:
    # Problem reading file
```

Sometimes a file can seem fine but the transcode was interrupted and no duration exists:
```python
cmd = ['mediainfo', '--Output=General;%Duration%', fpath]
check = subprocess.run(cmd, shell=False, capture_output=True, text=True)
if len(check.stderr) == 0:
    # File may be truncated!
```
---
class: contentpage
### **1.4 Validating your files**

The basic MediaConch policy 

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-19 at 17.21.35.png" width="1000">]

<font color="orange">Practise:</font> Check the transcode from your last encoding against this policy in the `validate_transcode` function of `command_builder.py`. The function returns a success or error string

```python3
success = cb.validate_transcode(outpath)
print(success)
```
---
class: contentpage
### **1.4 Validating your files**

There's also a MediaConch PyPi package with a very simple wrapper as an alternative to subprocess:
```python3
pip install mediaconch

from mediaconch import *
MC = MediaConch()

# Add the policy and file
MC.add_policy('basic_mp4_policy.xml')
file_id = MC.add_file('input.mp4')

# Set the format for the output
MC.set_format(Format.text)
print(MC.get_report(file_id))
```
Untested as yet!
---
class: contentpage
### **1.5 Capturing and using filter data**

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/blackdetect.gif" width="1000">]

The console logs show the blackdetect filter data captured as and when encountered in the encoding:

```sh
[blackdetect @ 0x13a808be0] black_start:196.24 black_end:207.28 black_duration:11.04
[blackdetect @ 0x13a808be0] black_start:369.84 black_end:374.84 black_duration:5 
[blackdetect @ 0x13a808be0] black_start:493.2 black_end:498.08 black_duration:4.88
```

---
class: contentpage
### **1.5 Capturing and using filter data**

This function converts the blackdetect data into a useable list of seconds:  

```python3
def get_blackspace_list(data) -> list:
    '''
    Return a list of start-end timings
    '''
    data_list = data.splitlines()
    time_range = []
    for line in data_list:
        if 'black_start' in line:
            split_line = line.split(":")
            split_start = split_line[1].split('.')[0]
            start = re.sub("[^0-9]", "", split_start)
            split_end = split_line[2].split('.')[0]
            end = re.sub("[^0-9]", "", split_end)
            end = str(int(end) + 1)
            time_range.append(f"{start} - {end}")
    return time_range
```
`[blackdetect @ 0x13a808be0] black_start:196.24 black_end:207.28 black_duration:11.04`  
`['196 - 208', '369 - 375', '493 - 499']`

---
class: contentpage
### **1.5 Capturing and using filter data**

The time_range list can now be checked for clashes with potential thumbnail frames:

```python3
def seconds_clash(time_ranges, second) -> bool:
    '''
    Create range and check for second within
    if clash found return True
    '''
    if not isinstance(second, int):
        second = int(second)

    for item in time_ranges:
        start, end = item.split(" - ")
        st = int(start) - 1
        ed = int(end) + 1
        if second in range(st, ed):
            print(f"Clash {second}: {item}")
            return True
```

The seconds argument needs to be supplied as an integer for use with `range()` but the `isinstance()` can ensure this is fixed if a string is passed. A True bool is returned if a match is made, otherwise the function will return a False.

---
class: contentpage
### **1.5 Capturing and using filter data**

<font color="orange">Practise:</font> Return to your REPL and using your `success` variable call `cb.get_black_space` to create your  `time_list`. Then pass `time_list` into `cb.seconds_clash` and see if you receive a True or False
```python
import command_builder as cb

# Experiment with width, height, DAR, PAR possibilities to see the commands that are returned
time_list = cb.get_blackspace_list(success.stdout)
print(time_list)
clash = cb.seconds_clash(time_list, '200')
print(f"Does this clash? {clash}")
```

NOTE: Check your `success` output is in the `stdout` argument returned from `subprocess.CompletedProcess`, sometimes I find it in `stderr` even though the `returncode` is 0
---
class: contentpage
### **1.5 Capturing and using filter data**

In case you are interested in making animated GIFs from your videos, here's the command I used:
```sh
ffmpeg -ss 2 -t 12 -i 'input.mov'
-vf "fps=10,scale=1000:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse" 
-loop 0 trimmed_animated.gif
```
---
class: contentpage
### ** 2. FFmpeg encoding optimisation**

Steps for enhancing FFmpeg encoding:

- Calculating processing speeds  
- Investigating hardware and networks  
- Types of processing approaches  
- Monitoring processes  
- Some open-source tools  
- GPU acceleration  


---
class: contentpage
### ** 2.1 Hardware configuration**

Recommendations from MediaArea for optimising your FFmpeg server:  
- Fast I/O communication speeds between your workstation and your storage (SSD or NAS) 
- Lots of RAM to bridge any bottleneck gaps from I/O interrupts  
- Ample CPU cores to manage processing  

There's a calculation from MediaArea to help you estimate processing speeds:
```
width  x  height  x  components  x  bitdepth  x  framerate
----------------------------------------------------------
1920   x  1080    x  3 (Y,U,V)   x  10        x  25        = ~ 1500 Mbps
4300   x  2920    x  3 (R,G,B)   x  16        x  24        = ~ 1800 Mbps
```

Realtime one pass processing the CPU will need to manage 1500 Mbps  
If for example you have 3 CPUs at 1 GHz each (processing 50Mbps):  
```
1500 Mbps  รท  50 Mbps  รท  3 Ghz  =  10 cores
1800 Mpbs  รท  50 Mbps  รท  3 Ghz  =  12 cores
```

I/O able to read and write fast enough for 1500/1800 Mbps ~ 200/225 MB/s  
Data converter sites like [unitconverters.net](https://www.unitconverters.net/data-transfer-converter.html) makes these calculations easier!

---
class: contentpage
### ** 2.1 Hardware configuration**

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-17 at 19.54.09.png" width="1000">]

```
Threads per core  x  Cores per socket  x  Sockets
--------------------------------------------------
       2          x        20          x    2     = 80 CPU     
```
---
class: contentpage
### ** 2.2 Processing**

.centre[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-20 at 12.32.45.png" width="1040">]


---
class: contentpage
### ** 2.2 Processing**

.centre[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-20 at 13.00.49.png" width="700">]

RAWcooked parallel encoding to FFV1 using multithreading

---
class: contentpage
### **2.3 GNU Parallel**

.left[<img src="https://raw.githubusercontent.com/digitensions/summer-school-2024-local/main/thursday/images/Screenshot 2024-09-17 at 21.23.59.png" width="380">]

GNU parallel can be installed with Homebrew for Mac, Unix or Windows Subsystem for Linux can use `apt-get`:
```sh
brew install parallel
sudo apt-get install parallel
```
There are extensive guides in their manual:
```sh
man parallel
```
---
class: contentpage
### **2.3 GNU Parallel**

GNU parallel is highly customisable with a very detailed manual, and online guides to help you optimise your use of it. But, with no arguments supplied at all Parallel will keep the CPUs operating at full capacity. 

This is a command used in RAWcooked encoding workflows at the BFI. As we launch multiple scripts against different storage we set a job maximum 3 to ensure all storage get equal processing:
```sh
grep ^N "${RPATH}list.txt" | parallel --jobs 3 \
  "rawcooked -y --all --no-accept-gaps ${RPATH}{} -o ${MPATH}{}.mkv &>> ${MPATH}{}.mkv.txt"
```

<font color="orange">Practise:</font> Change directory into one with files in. Pipe list of `.mkv` files from `ls` and create mediainfo logs for each named after the file:
```sh
ls | grep -E '.mkv$' | parallel 'mediainfo -i {} >> {}_mediainfo.log'
```
Use a `find` search to only select files modified more than 30 minutes ago and make MP4 encoded copies, and capture logs for the Parallel runs which include execution time:
```sh
find . -name '*.mov' -mmin 30 | parallel --jobs 3 --joblog \
'ffmpeg -i {} -c:v libx264 -c:a aac {}.mp4'
```
---
class: contentpage
### **2.4 Python parellelisation and multiprocessing**

A simple way to parallelise Python scripts is to pipe a list of file paths into GNU parallel then pass one path each to a Python script called by your `$PY3_ENV` environment version of `python3`:
```sh
grep '/mnt/' "${FPATH_LIST}" | parallel --jobs 10 "$PY3_ENV checksum_maker_mediainfo.py {}"
```
This example contains python code to create MD5 checksum and mediainfo reports for each file, but it could easily be used to encode with FFmpeg.  

Alternatively you can use Python's multiprocessing package, which allows code to spawn processes across multiple cores independently. The multiprocessing module has a `Pool()` class making this easily achievable:  
```python3
from multiprocessing import Pool

def main():
  with Pool() as p:
    response = p.map(ffmpeg_encode, file_list)
    print(f"Response code for encoding: {response.returncode}")
    p.close()
```
Iterate the `Pool()` class mapping `file_list` contents to the script's `ffmpeg_encode` function and allocating each to a separate processes, `p`.  

---
class: contentpage
### **2.4 Python parellelisation and multiprocessing**

<font color="orange">Practise:</font> Open the terminal and cd in the repository folder `thursday_scripts`. Identify a folder with several .mov or .mkv media files (duplicate a few examples if needed), then run the script supplying the folder path.

```sh
cd /SUMMER_SCHOOL_2024/thursday_scripts
python3 ffmpeg_multiprocess.py '/Users/joanna/test_files/'
```

If you have means, open `htop` and view the CPU activity as the script processes all your files in rapid succession.
---
class: contentpage
### **2.5 GPU acceleration** 

FFmpeg supports GPU acceleration for some codecs, and can significantly speed up video encoding and transcoding processes. It's particularly effective for real-time encoding and high-resolution video processing.  

GPU acceleration in FFmpeg with hardware-specific APIs:

- NVIDIA GPUs: NVENC (encoding) and NVDEC (decoding)
- AMD GPUs: AMF (Advanced Media Framework)
- Intel GPUs: QSV (Quick Sync Video)

To check for codecs with GPU support for your version of FFmpeg (not included with Package Manager installs):
```sh
ffmpeg -encoders | grep -E "nvenc|amf|qsv"
```

GPU acceleration works best with codecs that have hardware encoding support, like H.264, H.265 and VP9.

You will need to customise your FFmpeg installation so the codecs can be called from any automation, benefitting from the GPU accelerated encoding support.

FFV1 codec does not currently have GPU accelerated encoding support, but it is being planned!

[FFmpeg Intro to Hardware Acceleration](https://trac.ffmpeg.org/wiki/HWAccelIntro)

---
class: contentpage
### **2.5 GPU acceleration**

To use NVIDIA GPU acceleration for audiovisual encoding/transcoding:  

- You need an FFmpeg build with NVENC support and compatible NVIDIA drivers.
  Example command for H.264 encoding with NVIDIA GPU supported codec:
  ```sh
  ffmpeg -i input.mp4 -c:v h264_nvenc -preset slow -crf 23 -c:a copy output.mp4
  ```

To use AMD GPU acceleration:

- You need an FFmpeg build with AMF support. Example command for H.264 encoding with AMF GPU supported codec:
  ```sh
  ffmpeg -i input.mp4 -c:v h264_amf -quality balanced -c:a copy output.mp4
  ```

For Intel GPU acceleration:  

- You need an FFmpeg build with QSV support. Example command for H.264 encoding with Intel QSV supported codec:
  ```sh
  ffmpeg -i input.mp4 -c:v h264_qsv -preset slow -c:a copy output.mp4
  ```

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript"></script>
    <script type="text/javascript">var slideshow = remark.create({ratio: "16:9"});</script>
  </body>
</html>